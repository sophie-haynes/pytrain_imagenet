{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c75ddb-0e70-4b6e-afe5-f8567b5d1de4",
   "metadata": {},
   "source": [
    "# Evaluating Epoch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4599dc61-c591-482e-8a30-7089590abdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision.models import resnet50\n",
    "import torchvision\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "# set root dirs\n",
    "grey_dir = \"/home/local/data/sophie/imagenet/output/grey\"\n",
    "base_dir = \"/home/local/data/sophie/imagenet/output/base\"\n",
    "# configure GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# fetch imagenet classes \n",
    "with open('/home/local/data/sophie/imagenet/imagenet_classes.pkl', 'rb') as f:\n",
    "    clses = pickle.load(f)\n",
    "\n",
    "# define data augmentations\n",
    "img_size = 224\n",
    "transforms = torchvision.transforms.Compose([\n",
    "      torchvision.transforms.RandomCrop(img_size, padding=random.randint(0, 8)), # jitter\n",
    "      torchvision.transforms.RandomRotation((-45, 45)), # rotate\n",
    "      torchvision.transforms.RandomResizedCrop(img_size, scale=(0.9, 1.2), ratio=(1.0, 1.,0)) # scale\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_models(base_model_path,grey_model_path):\n",
    "    # initalize models\n",
    "    base_model = resnet50(pretrained=True)\n",
    "    grey_model = resnet50(pretrained=True)\n",
    "\n",
    "    # load weights\n",
    "    base_weights = torch.load(base_model_path, map_location='cpu')\n",
    "    # configure state dict\n",
    "    new_state_dict = {}\n",
    "    for k, v in base_weights['model'].items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "    # load model with state dict\n",
    "    base_model.load_state_dict(new_state_dict)\n",
    "    # disable grad\n",
    "    for param in base_model.parameters():\n",
    "      param.requires_grad_(False)\n",
    "    \n",
    "    # load weights\n",
    "    grey_weights = torch.load(grey_model_path, map_location='cpu')\n",
    "    # configure state dict\n",
    "    new_state_dict = {}\n",
    "    for k, v in grey_weights['model'].items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "    # load model with state dict\n",
    "    grey_model.load_state_dict(new_state_dict)\n",
    "    # disable grad\n",
    "    for param in grey_model.parameters():\n",
    "      param.requires_grad_(False)\n",
    "\n",
    "    # send to GPU\n",
    "    base_model.to(device)\n",
    "    grey_model.to(device)\n",
    "    # set as eval \n",
    "    base_model.eval()\n",
    "    grey_model.eval()\n",
    "    return base_model, grey_model\n",
    "    \n",
    "def visualize_neuron(model, layer, neuron_indices, img_shape=(3, 224, 224), iterations=30, lr=0.1, device='cuda', transforms=None):\n",
    "    # Initialize the input image with requires_grad=True\n",
    "    input_img = torch.randn(1, *img_shape, requires_grad=True, device=device)\n",
    "    # input_img = torch.randn(1, *img_shape, requires_grad=True, device='cpu')\n",
    "    optimizer = torch.optim.Adam([input_img], lr=lr)\n",
    "\n",
    "    activations = None\n",
    "\n",
    "    # Non-intrusive hook function that captures activations\n",
    "    def non_intrusive_hook_fn(module, input, output):\n",
    "        nonlocal activations\n",
    "        activations = output  # Do not detach to keep the graph intact\n",
    "\n",
    "    # Register the hook to capture the layer's output\n",
    "    hook = layer.register_forward_hook(non_intrusive_hook_fn)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if transforms is not None:\n",
    "            trs_img = transforms(input_img)\n",
    "        else:\n",
    "            trs_img = input_img\n",
    "\n",
    "        # Forward pass\n",
    "        model(trs_img)\n",
    "\n",
    "        # Ensure that neuron_indices is within bounds\n",
    "        if neuron_indices >= activations.shape[1]:\n",
    "            raise ValueError(f\"neuron_indices {neuron_indices} is out of bounds for activations with shape {activations.shape}\")\n",
    "\n",
    "        # Loss calculation as in Method 2\n",
    "        if activations.dim() == 2:\n",
    "            channel_activations = activations[:, neuron_indices]\n",
    "        else:\n",
    "            channel_activations = activations[:, neuron_indices, :, :]\n",
    "\n",
    "        loss = -channel_activations.mean()\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Remove the hook\n",
    "    hook.remove()\n",
    "\n",
    "    return input_img.detach()\n",
    "\n",
    "def generateClassImages(base_model, grey_model, neuron, its=3000, lr=0.05, layer_name='fc', seed=0,trs=transforms):\n",
    "    # get the target layers\n",
    "    colour_layer1 = dict([*base_model.named_modules()])[layer_name]\n",
    "    grey_layer1 = dict([*grey_model.named_modules()])[layer_name]\n",
    "    # generate images\n",
    "    colour_mean_img1 = visualize_neuron(base_model, colour_layer1, neuron, iterations=its, \n",
    "                                        lr=lr, transforms=trs, device=device)\n",
    "    grey_mean_img1 = visualize_neuron(grey_model, grey_layer1, neuron, iterations=its, \n",
    "                                      lr=lr, transforms=trs, device=device)\n",
    "    # normalise images\n",
    "    colour_mean_img1_norm = (colour_mean_img1.cpu().numpy()-colour_mean_img1.cpu().numpy().min())/(colour_mean_img1.cpu().numpy().max()-colour_mean_img1.cpu().numpy().min())\n",
    "    grey_mean_img1_norm = (grey_mean_img1.cpu().numpy()-grey_mean_img1.cpu().numpy().min())/(grey_mean_img1.cpu().numpy().max()-grey_mean_img1.cpu().numpy().min())\n",
    "    \n",
    "    return colour_mean_img1, colour_mean_img1_norm, grey_mean_img1, grey_mean_img1_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d65e86-d4f3-44ec-a9f8-70c7c0ed3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 87.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "neuron = 963\n",
    "img_class = clses[neuron].split(\",\")[0]\n",
    "plt_dir = \"/home/jovyan/pytrain_imagenet/viz_plots/{}\".format(img_class)\n",
    "if not os.path.exists(plt_dir):\n",
    "    os.makedirs(plt_dir)\n",
    "\n",
    "# iterate over models\n",
    "for model in os.listdir(grey_dir):\n",
    "    if \"model\" in model:\n",
    "        # set current model paths\n",
    "        grey_model_path = os.path.join(grey_dir, model)\n",
    "        base_model_path = os.path.join(base_dir, model.replace(\"grey\", \"base\"))\n",
    "        # load models\n",
    "        base_model, grey_model = load_models(grey_model_path, base_model_path)\n",
    "        \n",
    "        colour_image, norm_colour_image, grey_image, norm_grey_image, = generateClassImages(base_model, grey_model, neuron)\n",
    "        \n",
    "        # generate plot\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(norm_colour_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Colour FC: {}\".format(img_class))\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(norm_grey_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grey FC: {}\".format(img_class))\n",
    "        plt.savefig(os.path.join(plt_dir, \"{}.png\".format(model.split(\"_\")[-1].split(\".\")[0])), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908c8ec-6d42-4751-a33a-f94877366397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24538/2528474903.py:19: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10,5))\n"
     ]
    }
   ],
   "source": [
    "neuron = 388\n",
    "img_class = clses[neuron].split(\",\")[0]\n",
    "plt_dir = \"/home/jovyan/pytrain_imagenet/viz_plots/{}\".format(img_class)\n",
    "if not os.path.exists(plt_dir):\n",
    "    os.makedirs(plt_dir)\n",
    "\n",
    "# iterate over models\n",
    "for model in os.listdir(grey_dir):\n",
    "    if \"model\" in model:\n",
    "        # set current model paths\n",
    "        grey_model_path = os.path.join(grey_dir, model)\n",
    "        base_model_path = os.path.join(base_dir, model.replace(\"grey\", \"base\"))\n",
    "        # load models\n",
    "        base_model, grey_model = load_models(grey_model_path, base_model_path)\n",
    "        \n",
    "        colour_image, norm_colour_image, grey_image, norm_grey_image, = generateClassImages(base_model, grey_model, neuron)\n",
    "        \n",
    "        # generate plot\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(norm_colour_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Colour FC: {}\".format(img_class))\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(norm_grey_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grey FC: {}\".format(img_class))\n",
    "        plt.savefig(os.path.join(plt_dir, \"{}.png\".format(model.split(\"_\")[-1].split(\".\")[0])), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8da45-776d-4171-bcac-7ed94ac7375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 470\n",
    "img_class = clses[neuron].split(\",\")[0]\n",
    "plt_dir = \"/home/jovyan/pytrain_imagenet/viz_plots/{}\".format(img_class)\n",
    "if not os.path.exists(plt_dir):\n",
    "    os.makedirs(plt_dir)\n",
    "\n",
    "# iterate over models\n",
    "for model in os.listdir(grey_dir):\n",
    "    if \"model\" in model:\n",
    "        # set current model paths\n",
    "        grey_model_path = os.path.join(grey_dir, model)\n",
    "        base_model_path = os.path.join(base_dir, model.replace(\"grey\", \"base\"))\n",
    "        # load models\n",
    "        base_model, grey_model = load_models(base_model_path,grey_model_path)\n",
    "        \n",
    "        colour_image, norm_colour_image, grey_image, norm_grey_image, = generateClassImages(base_model, grey_model, neuron)\n",
    "        \n",
    "        # generate plot\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(norm_colour_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Colour FC: {}\".format(img_class))\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(norm_grey_image[0].transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grey FC: {}\".format(img_class))\n",
    "        plt.savefig(os.path.join(plt_dir, \"{}.png\".format(model.split(\"_\")[-1].split(\".\")[0])), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44665f48-ace4-4d5b-bb4e-cf42ba35194f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
